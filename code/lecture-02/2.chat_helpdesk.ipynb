{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Process scrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "tqdm.pandas()\n",
    "\n",
    "from chat.constants import EMBEDDING_MODEL_REPO_NAME, PROCESSED_DATA_FILE\n",
    "\n",
    "# note: this path is different from chat.constants, because now we are executing notebook from a different folder\n",
    "DATA_FOLDER = Path('./tmp/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Process the html into texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Replace sequences of spaces and carriage returns surrounded by newlines with a single newline\n",
    "    text = re.sub(r'(\\n)[\\s\\r]+(\\n)', '\\n', text)\n",
    "    \n",
    "    # Replace sequences of multiple newlines with a single newline\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    \n",
    "    # Replace sequences of spaces (not surrounded by newlines) with a single space\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def parse_html(x):\n",
    "    try:\n",
    "        soup = BeautifulSoup(x, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def process_scraped_data(input_path, output_folder, remove_string_prefix=None, remove_string_suffix=None):\n",
    "    # output_path = Path(output_path)\n",
    "    output_path = output_folder / PROCESSED_DATA_FILE\n",
    "    if not output_path.exists():        \n",
    "        logging.info(\"Loaded scraped data\")\n",
    "        res = pickle.load(open(input_path, \"rb\"))\n",
    "        logging.info(\"Convert to pandas\")\n",
    "        df_urls = pd.DataFrame([\n",
    "            {\n",
    "                \"url\": v[\"metadata\"][\"url\"], \n",
    "                \"timestamp\": v[\"metadata\"][\"timestamp\"], \n",
    "                \"content\": v[\"content\"],\n",
    "                \"key\": v[\"key\"]\n",
    "            } for v in res.values()]).sort_values(\"timestamp\")\n",
    "        \n",
    "        logging.info(\"Remove urls\")\n",
    "        mask = np.ones(len(df_urls), dtype=bool)\n",
    "        if remove_string_prefix is not None:\n",
    "            logging.info(f\"Removing urls with prefix: {', '.join(remove_string_prefix)}\")\n",
    "            mask &= df_urls.url.apply(lambda x: not any(x.startswith(s) for s in remove_string_prefix)).values\n",
    "        if remove_string_suffix is not None:\n",
    "            logging.info(f\"Removing urls with suffix: {', '.join(remove_string_suffix)}\")\n",
    "            mask &= df_urls.url.apply(lambda x: not any(x.endswith(s) for s in remove_string_suffix)).values    \n",
    "        df_data = df_urls[mask].reset_index(drop=True).copy()\n",
    "\n",
    "        logging.info(\"Parse html\")\n",
    "        df_data[\"text\"] = df_data[\"content\"].progress_apply(parse_html)\n",
    "        df_data.dropna(subset=[\"text\"], inplace=True)\n",
    "\n",
    "        logging.info(\"Clean text\")\n",
    "        df_data[\"text_cleaned\"] = df_data.text.progress_apply(clean_text)\n",
    "        \n",
    "        logging.info(\"Save to parquet\")\n",
    "        df_data.reset_index(drop=True).to_parquet(output_path)\n",
    "        \n",
    "    return pd.read_parquet(output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = DATA_FOLDER / \"scraped_data.pkl\"\n",
    "df_data = process_scraped_data(input_path, DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url             https://helpdesk.ugent.be/security/veilig-werk...\n",
       "timestamp                              2023-10-22T22:22:12.916451\n",
       "content         \\n<!doctype html>\\n<html lang=\"nl\">\\n<head>\\n ...\n",
       "text            \\n\\n\\n\\n\\nVeilig werken met IT aan de UGent (m...\n",
       "text_cleaned    Veilig werken met IT aan de UGent (medewerkers...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data[[\"url\", \"timestamp\", \"content\", \"text\", \"text_cleaned\"]].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. transform data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Callable\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from chat.constants import CHUNK_OVERLAP, CHUNKED_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Tokenizer:\n",
    "    chunk_overlap: int\n",
    "    tokens_per_chunk: int\n",
    "    decode: Callable[[List[int]], str]\n",
    "    encode: Callable[[str], List[int]]\n",
    "\n",
    "def split_text_on_tokens(*, text: str, tokenizer: Tokenizer):\n",
    "    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\n",
    "    splits: List[str] = []\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    start_idx = 0\n",
    "    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n",
    "    chunk_ids = input_ids[start_idx:cur_idx]\n",
    "    while start_idx < len(input_ids):\n",
    "        splits.append(tokenizer.decode(chunk_ids))\n",
    "        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\n",
    "        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n",
    "        chunk_ids = input_ids[start_idx:cur_idx]\n",
    "    return splits\n",
    "\n",
    "def chunk_text(df_data, repo_name, output_folder):\n",
    "    output_file_path = output_folder / CHUNKED_FILE\n",
    "    if not output_file_path.exists():\n",
    "        model_tokenizer = AutoTokenizer.from_pretrained(repo_name)\n",
    "        \n",
    "        chunk_size = model_tokenizer.model_max_length\n",
    "            \n",
    "        tokenizer = Tokenizer(chunk_overlap=CHUNK_OVERLAP, tokens_per_chunk=chunk_size,decode=lambda x: model_tokenizer.decode(x, skip_special_tokens=True), encode=model_tokenizer.encode)\n",
    "\n",
    "        df_data[\"text_chunked\"] = df_data.text_cleaned.progress_apply(lambda x: split_text_on_tokens(text=x, tokenizer=tokenizer))\n",
    "        df_data[['key', 'text_chunked']].to_parquet(output_file_path)    \n",
    "    df_data = pd.read_parquet(output_file_path)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_chunked = chunk_text(df_data, EMBEDDING_MODEL_REPO_NAME, DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "key                              4d36516f1ec9bbee4576cc75195f926b\n",
       "text_chunked    [Veilig werken met IT aan de UGent (medewerker...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_chunked.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. embed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "from chat.constants import EMBEDDING_PROMPT, EMBEDDING_MODEL_DEVICE, CHUNKED_EMB_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def get_embedding(model, tokenizer, text, pooling=\"mean\"):\n",
    "    def average_pool(last_hidden_states, attention_mask):\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        return (last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]).cpu().numpy()\n",
    "    \n",
    "    batch_dict = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt').to(model.device)\n",
    "    outputs = model(**batch_dict)\n",
    "\n",
    "    if pooling == \"mean\":\n",
    "        return average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    elif pooling == \"last\":\n",
    "        return outputs.last_hidden_state[:, -1, :]\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{pooling} pooling not implemented\")\n",
    "\n",
    "def embed_texts(text, model, tokenizer, pooling, batch_size=128):\n",
    "    def chunk(it, size):\n",
    "        iterator = iter(it)\n",
    "        while chunk := list(islice(iterator, size)):\n",
    "            yield chunk\n",
    "\n",
    "    embeddings = []    \n",
    "    for batch in tqdm(chunk(text, batch_size), total=len(text) // batch_size +1):            \n",
    "        emb = get_embedding(model, tokenizer, batch, pooling=pooling)\n",
    "        embeddings.append(emb)\n",
    "    return list(np.vstack(embeddings).astype(float))\n",
    "\n",
    "def embed_chunks(repo_name, output_folder):\n",
    "    output_file_path = output_folder / CHUNKED_EMB_FILE\n",
    "    if not output_file_path.exists():\n",
    "        assert df_data[\"key\"].nunique() == len(df_data)\n",
    "        df_chunks_exploded = df_data.explode('text_chunked').reset_index(drop=True).reset_index().rename(columns={'index': 'chunk_id'})\n",
    "\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(repo_name)\n",
    "        model = AutoModel.from_pretrained(repo_name)\n",
    "        model.eval()\n",
    "        model.cuda(EMBEDDING_MODEL_DEVICE)\n",
    "\n",
    "        # add prompt\n",
    "        texts = df_chunks_exploded.text_chunked.progress_apply(lambda x: EMBEDDING_PROMPT.format(text=x)).values\n",
    "        df_chunks_exploded[\"emb\"] = embed_texts(texts, model, tokenizer, pooling=\"mean\")\n",
    "        df_chunks_exploded.to_parquet(output_file_path)\n",
    "\n",
    "    df_chunk_embs = pd.read_parquet(output_file_path)\n",
    "    return df_chunk_embs\n",
    "\n",
    "df_chunk_embs = embed_chunks(EMBEDDING_MODEL_REPO_NAME, DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat.constants import RETRIEVAL_TOP_K\n",
    "from chat.utils import get_corpus, get_corpus_embeddings, dense_retrieval, get_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 similarity between embedded chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Veilig werken met IT aan de UGent (medewerkers...\n",
       "6      Veilig werken met IT aan de UGent (medewerkers...\n",
       "982    Working safely with IT at the UGent (students)...\n",
       "972    Working safely with IT at the UGent (staff) ZO...\n",
       "3      slagen gegevens versleuteld worden (bv. met Bi...\n",
       "Name: text_chunked, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_id = 0\n",
    "\n",
    "# get query embedding\n",
    "query_emb = df_chunk_embs.iloc[doc_id][\"emb\"]\n",
    "\n",
    "# get corpus embeddings\n",
    "corpus_embs = np.vstack(df_chunk_embs[\"emb\"].values)\n",
    "corpus_embs = corpus_embs / np.linalg.norm(corpus_embs, axis=1, keepdims=True)\n",
    "\n",
    "# compute cosine similarity\n",
    "rec_ids = np.argsort(np.dot(corpus_embs, query_emb))[-RETRIEVAL_TOP_K:][::-1]\n",
    "\n",
    "# get retrieved texts\n",
    "df_chunk_embs.iloc[rec_ids][\"text_chunked\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Manually Check User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading embeddings\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Loading corpus\")\n",
    "df_chunk_embs = get_corpus(DATA_FOLDER)\n",
    "\n",
    "logging.info(\"Loading embeddings\")    \n",
    "chunk_embs = get_corpus_embeddings(df_chunk_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_TOKENIZER = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_REPO_NAME)\n",
    "EMBEDDING_MODEL = AutoModel.from_pretrained(EMBEDDING_MODEL_REPO_NAME)\n",
    "EMBEDDING_MODEL.eval()\n",
    "_ = EMBEDDING_MODEL.cuda(EMBEDDING_MODEL_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_msg = \"How can I set up email on MacOS?\"\n",
    "dense_retrieval_results = dense_retrieval(user_msg, chunk_embs, EMBEDDING_MODEL, EMBEDDING_TOKENIZER, RETRIEVAL_TOP_K)\n",
    "candidate_keys, candidate_docs = get_documents(dense_retrieval_results, df_chunk_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['E-mail - Set up on macOS for Exchange Online (Office 365) ZOEK MENU Search In het Nederlands UGentNetE-mailAccount & passwordAthena & softwareStorageHelpMe DICT Helpdesk HomeE-mailMacos E-mail - Set up on macOS On this page Mail Microsoft Outlook On macOS (Monterey - version 12.x.x) you can add an e-mail account in the standard e-mail program Mail or in Microsoft Outlook (part of Office 365). Mail Choose \"System Preferences\" from the Apple menu Go to \"Internet Accounts\" Choose \"Microsoft Exchange\" in the right panel Enter your name and UGent e-mail address and choose \"Sign In\" Choose \"Sign In\" again when asked if you want to sign in to your Exchange account using Microsoft Enter your password on the UGent login screen Approve the sign-in request with your 2nd factor Select the apps you want to use with your account and choose \"Done\" The Exchange account is now available in Mail Microsoft Outlook Choose \"Preferences...\" from the Outlook menu Go to \"Accounts\" Add a new account using the + symbol at the bottom of the left panel Enter your UGent e-mail address in the next screen and choose \"Continue\" Enter your password on the UGent login screen Confirm the application request with your 2nd factor Choose \"Done\" on the confirmation screen and close the preferences window Your account is now available in the navigation panel on the left Account & password About your UGent account Logging in to your UGent account Forgot your password Change password E-mail Using webmail, no specific settings needed Setting up an e-mail application E-mail abuse: spam, phishing, spoofing UGentNet Wireless connection: use Eduroam Via cable in UGent buildings (for faculties and departments) Connecting from locations outside of the UGent network Athena & software Overview Installing Citrix for first use Use athena on your mobile device Microsoft Office Twitter Feedback Disclaimer Cookies © 2023 Universiteit Gent',\n",
       "       'E-mail - iOS (iPhone/iPad) configuration for Exchange Online (Office 365) ZOEK MENU Search In het Nederlands UGentNetE-mailAccount & passwordAthena & softwareStorageHelpMe DICT Helpdesk HomeE-mailIpad Email - iOS (iPhone/iPad) configuration On this page Outlook Mail app Outlook Make the navigation column visible by swiping from left to right Go to the settings (cogwheel at the bottom) Choose \"Add email account\" under Email accounts Choose \"Add e-mail account\" in the menu that now appears Enter your UGent email address and choose \"Add account\" Choose \"Open Verificator\" in the next screen Enter your UGent password and confirm the login request with your 2nd factor Close the settings Your UGent email account is now visible in the navigation columnYour UGent email account is now visible in the navigation column Mail app Go to \"Settings\" > \"Mail\" Choose \"Accounts\" Choose \"New Account\" Choose \"Microsoft Exchange\" Enter: E-mail: your UGent e-mail address Description: (choose a description yourself, e.g. \"UGent\") Choose \"Next\" You will be asked if you want to login to your Exchange account \"ugent.be\" via Microsoft Choose \"Log in\" Enter your UGent-password and confirm the login request with your 2nd factor In the last screen you can select which parts of your Exchange account you want to synchronize, choose at least \"Mail\" Choose \"Save\" Your UGent email account is now available in your Mail app top Account & password About your UGent account Logging in to your UGent account Forgot your password Change password E-mail Using webmail, no specific settings needed Setting up an e-mail application E-mail abuse: spam, phishing, spoofing UGentNet Wireless connection: use Eduroam Via cable in UGent buildings (for faculties and departments) Connecting from locations outside of the UGent network Athena & software Overview Installing Citrix for first use Use athena on your mobile device Microsoft Office Twitter Feedback Disclaimer Cookies © 2023 Universiteit Gent',\n",
       "       'mail, no specific settings needed Setting up an e-mail application E-mail abuse: spam, phishing, spoofing UGentNet Wireless connection: use Eduroam Via cable in UGent buildings (for faculties and departments) Connecting from locations outside of the UGent network Athena & software Overview Installing Citrix for first use Use athena on your mobile device Microsoft Office Twitter Feedback Disclaimer Cookies © 2023 Universiteit Gent',\n",
       "       'configuring your email app? Check https://helpdesk.ugent.be/email/en/ Created: 22 August 2023 Last updated: 22 August 2023 10:36:04 Account & password About your UGent account Logging in to your UGent account Forgot your password Change password E-mail Using webmail, no specific settings needed Setting up an e-mail application E-mail abuse: spam, phishing, spoofing UGentNet Wireless connection: use Eduroam Via cable in UGent buildings (for faculties and departments) Connecting from locations outside of the UGent network Athena & software Overview Installing Citrix for first use Use athena on your mobile device Microsoft Office Twitter Feedback Disclaimer Cookies © 2023 Universiteit Gent',\n",
       "       'stel Microsoft Office Twitter Feedback Disclaimer Cookies © 2023 Universiteit Gent'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ugenai2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
