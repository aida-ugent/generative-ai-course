{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# hugging face model repo\n",
    "MODEL_REPO = \"lmsys/vicuna-7b-v1.5\"\n",
    "# cuda device\n",
    "CUDA_DEVICE = \"cuda:0\"\n",
    "# local cache dir\n",
    "MODEL_CACHE_DIR = Path(\"./tmp/models/\")\n",
    "# create cache dir\n",
    "MODEL_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# max token length\n",
    "MAX_LENGTH = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d92765dc9341e7a80af4c6e7bd4b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model, if not exist, download from model repo\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_REPO, cache_dir=MODEL_CACHE_DIR)\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_REPO, cache_dir=MODEL_CACHE_DIR, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "    model.to(CUDA_DEVICE)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Ghent University?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:\n",
      "What is Ghent University?\n",
      "\n",
      "Iteration 1:\n",
      "What is Ghent University?\n",
      " nobody\n",
      "Iteration 2:\n",
      "What is Ghent University?\n",
      " nobody knows\n",
      "Iteration 3:\n",
      "What is Ghent University?\n",
      " nobody knows\n"
     ]
    }
   ],
   "source": [
    "# This code is used to generate text based on an initial input query using a language model.\n",
    "\n",
    "# Disable gradient computation. This is done to save memory and speed as we only want inference (no backpropagation).\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Get the length of the initial input query.\n",
    "    l_prompt = len(query)\n",
    "\n",
    "    # Set the temperature for sampling. A lower value makes the output more deterministic.\n",
    "    temperature = 0.0\n",
    "\n",
    "    # Set the maximum number of tokens the model can generate in one forward pass.\n",
    "    max_new_tokens = MAX_LENGTH\n",
    "\n",
    "    # Tokenize the input query to get the input IDs (numeric representation of the input text).\n",
    "    input_ids = tokenizer(query).input_ids\n",
    "\n",
    "    # Initialize the list of output IDs with the input IDs.\n",
    "    output_ids = list(input_ids)\n",
    "\n",
    "    # Calculate the maximum source length to ensure the input doesn't exceed the model's max sequence length.\n",
    "    max_src_len = 4096 - max_new_tokens - 8\n",
    "    input_ids = input_ids[-max_src_len:]\n",
    "\n",
    "    # Loop over the maximum number of tokens to generate.\n",
    "    for i in range(max_new_tokens):\n",
    "\n",
    "        # If it's the first token to be generated.\n",
    "        if i == 0:\n",
    "            # Pass the input IDs to the model to get logits and past key values.\n",
    "            out = model(\n",
    "                torch.as_tensor([input_ids]).to(CUDA_DEVICE), use_cache=True)\n",
    "            logits = out.logits\n",
    "            past_key_values = out.past_key_values\n",
    "\n",
    "        # For subsequent tokens.\n",
    "        else:\n",
    "            # Create an attention mask to pay attention to the current tokens.\n",
    "            attention_mask = torch.ones(\n",
    "                1, past_key_values[0][0].shape[-2] + 1, device=CUDA_DEVICE)\n",
    "            \n",
    "            # Pass the last generated token and past key values to the model.\n",
    "            out = model(input_ids=torch.as_tensor([[token]], device=CUDA_DEVICE),\n",
    "                        use_cache=True,\n",
    "                        attention_mask=attention_mask,\n",
    "                        past_key_values=past_key_values)\n",
    "            logits = out.logits\n",
    "            past_key_values = out.past_key_values\n",
    "\n",
    "        # Extract the logits for the last token.\n",
    "        last_token_logits = logits[0][-1]\n",
    "\n",
    "        # If temperature is very low, pick the token with the highest logit directly.\n",
    "        if temperature < 1e-4:\n",
    "            token = int(torch.argmax(last_token_logits))\n",
    "        # Otherwise, use temperature sampling.\n",
    "        else:\n",
    "            probs = torch.softmax(last_token_logits / temperature, dim=-1)\n",
    "            token = int(torch.multinomial(probs, num_samples=1))\n",
    "\n",
    "        # Append the generated token to the output IDs.\n",
    "        output_ids.append(token)\n",
    "\n",
    "        # Check if the generated token is the end-of-sequence token.\n",
    "        if token == tokenizer.eos_token_id:\n",
    "            stopped = True\n",
    "        else:\n",
    "            stopped = False\n",
    "\n",
    "        # Decode the output IDs to get the generated text.\n",
    "        output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "        print(f\"Iteration {i}:\\n{output}\")\n",
    "\n",
    "        # If the generation should stop, break out of the loop.\n",
    "        if stopped:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query use model's generate method\n",
    "understand the parameters: https://huggingface.co/blog/how-to-generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Ghent University?\n",
      "Ghent University is a public research university located in Ghent, Belgium. It was founded in 1817 and is one of the oldest and most prestigious universities in the country. The university has a strong focus on research and has made significant contributions to a wide range of fields, including medicine, engineering, social sciences, and humanities.\n",
      "\n",
      "Ghent University has three campuses located in Ghent, and it offers a wide range of undergraduate and graduate programs in various fields of study. The university has a diverse student body, with students from over 120 different countries.\n",
      "\n",
      "The university is known for its high-quality education and research, and it has a strong reputation in the academic community. Ghent University is also home to several research centers and institutes, including the Ghent Institute for Biotechnology, the Ghent Center for Conflict and Security Studies, and the Ghent Institute for International Studies.\n",
      "\n",
      "Overall, Ghent University is a leading educational and research institution in Belgium and is recognized for its excellence in teaching, research, and innovation.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = tokenizer(query, padding=False, add_special_tokens=False, return_tensors=\"pt\").to(CUDA_DEVICE)    \n",
    "    output_sequences = model.generate(input_ids=inputs[\"input_ids\"], max_length=MAX_LENGTH,  do_sample=True, temperature=0.9, top_p=0.6)\n",
    "    text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query use pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Ghent University?\n",
      " Ghent University is a public research university located in Ghent, Belgium. It was founded in 1817 and is one of the oldest and most prestigious universities in Belgium. The university has 11 faculties and offers a wide range of undergraduate and graduate programs in various fields such as science, engineering, medicine, law, economics, and social sciences. Ghent University is known for its high-quality education, innovative research, and strong international collaboration. It has a diverse student body and a faculty of over 6,000 employees. The university is also home to several research institutes and centers, including the Ghent Institute for Biotechnology and the Ghent Center for Conflict and Development Studies.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chat_pipeline = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=MAX_LENGTH, device=CUDA_DEVICE)\n",
    "result = chat_pipeline(query, do_sample=True, temperature=0.9, top_p=0.6)\n",
    "text = result[0][\"generated_text\"]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the LLM into a LangChain compatible model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/modules/model_io/models/llms/custom_llm\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    model_name = MODEL_REPO    \n",
    "    \n",
    "    def __init__(self, *args, **kwargs):        \n",
    "        super().__init__(*args, **kwargs)\n",
    "        object.__setattr__(self, 'model_pipeline', chat_pipeline)\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "    ) -> str:        \n",
    "        result = self.model_pipeline(prompt, do_sample=True, temperature=0.9, top_p=0.6)\n",
    "        text = result[0][\"generated_text\"]\n",
    "        return text\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"model_name\": self.model_name}\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"    \n",
    "llm = CustomLLM()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\n",
    "    \"Please list three facts about Ghent university.\", \n",
    "    \"Please repeat the second fact.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query without memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0: Please list three facts about Ghent university.\n",
      "Response: Please list three facts about Ghent university. (1) Ghent University is a public research university located in Ghent, Belgium. (2) It was founded in 1905 and is one of the oldest universities in Belgium. (3) The university has a strong focus on research and has a number of research centers and institutes, including the Ghent Institute for Biotechnology and the Ghent Center for Conflict and Security Studies.\n",
      "\n",
      "\n",
      "Query 1: Please repeat the second fact.\n",
      "Response: Please repeat the second fact.1. The sun is the star at the center of the solar system.2. The sun is the star at the center of the solar system.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    # avoid the sequential using pipelines warning\n",
    "    warnings.simplefilter('ignore')\n",
    "    for qid in range(2):\n",
    "        print(f\"Query {qid}: {query[qid]}\")\n",
    "        response = llm(query[qid])\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that \"chat_history\" is present in the prompt template\n",
    "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "New human query: {question}\n",
    "Response:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0: Please list three facts about Ghent university.\n",
      "Response: 1. Ghent University is located in Ghent, Belgium.\n",
      "2. It was founded in 1817 and is one of the oldest universities in Belgium.\n",
      "3. Ghent University has a strong focus on research and innovation, with over 150 research groups and a wide range of collaborations with industry and other institutions.\n",
      "\n",
      "\n",
      "Query 1: Please repeat the second fact.\n",
      "Response: 2. Ghent University was founded in 1817 and is one of the oldest universities in Belgium.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Notice that we need to align the `memory_key`\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    for qid in range(2):\n",
    "        print(f\"Query {qid}: {query[qid]}\")\n",
    "        result = conversation({\"question\": query[qid]})[\"text\"]\n",
    "        print(f\"Response: {result.split('Response:')[-1].strip()}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import ServiceContext, LangchainEmbedding, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms import CustomLLM, CompletionResponse, CompletionResponseGen, LLMMetadata\n",
    "from llama_index.llms.base import llm_completion_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-large\"\n",
    "CONTEXT_WINDOW = 4096\n",
    "NUM_OUTPUT = 2048\n",
    "DEFAULT_BATCH_SIZE = 512\n",
    "\n",
    "data_folder = Path(\"./tmp/examples/llama_index/data\")\n",
    "Path(data_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# download data if not yet downloaded\n",
    "# ! wget https://raw.githubusercontent.com/run-llama/llama_index/main/examples/paul_graham_essay/data/paul_graham_essay.txt -P ./tmp/examples/llama_index/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the Langchain LLM into a llama_index LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaIndexLLM(CustomLLM):\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=CONTEXT_WINDOW,\n",
    "            num_output=NUM_OUTPUT,\n",
    "            model_name=MODEL_REPO\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        prompt_length = len(prompt)             \n",
    "        response = chat_pipeline(prompt, do_sample=True, temperature=0.9, top_p=0.6, max_length=CONTEXT_WINDOW)[0][\"generated_text\"]\n",
    "        # only return newly generated tokens\n",
    "        text = response[prompt_length:]\n",
    "        return CompletionResponse(text=text)\n",
    "    \n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "llm = LlamaIndexLLM()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the index for querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_kwargs={'device': CUDA_DEVICE},        \n",
    "    ),\n",
    "    embed_batch_size=DEFAULT_BATCH_SIZE\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    embed_model=embed_model,\n",
    "    context_window=CONTEXT_WINDOW, \n",
    "    num_output=NUM_OUTPUT,    \n",
    ")\n",
    "documents = SimpleDirectoryReader(data_folder).load_data()\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d6c925da6948779eb4b268d4997a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.indices.utils:> Top 2 nodes:\n",
      "> [Node a5acc068-1893-4bc4-9307-7745545afeb6] [Similarity score:             0.798015] What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college the two main things I worked on, outside of schoo...\n",
      "> [Node f4a2c494-1dd4-4666-a8d8-53ddd8638d6c] [Similarity score:             0.796443] Now all I had to do was learn Italian.\n",
      "\n",
      "Only stranieri (foreigners) had to take this entrance exa...\n",
      "DEBUG:llama_index.llm_predictor.base:\n",
      "The author grew up writing short stories and trying to learn computer programming. He didn't start studying philosophy in college as he initially planned, but instead switched to AI. He also had an interest in painting and attended the Accademia di Belle Arti in Florence, Italy, where he painted still lives at night in his bedroom.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "# print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ugenai2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
